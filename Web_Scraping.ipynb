{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNikWh5wLz8jg6rjpXvK3Rm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Afroze07/ChatGPT-Voice-Assistant/blob/main/Web_Scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NN0wPkEKAGbx",
        "outputId": "6755f97e-7e0c-44ef-a840-772d337d265e"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scraping page 1\n",
            "Scraping page 2\n",
            "Scraping page 3\n",
            "Scraping page 4\n",
            "Scraping page 5\n",
            "Scraping page 6\n",
            "Scraping page 7\n",
            "Scraping page 8\n",
            "Scraping page 9\n",
            "Scraping page 10\n",
            "Scraping page 11\n",
            "Scraping page 12\n",
            "Scraping page 13\n",
            "Scraping page 14\n",
            "Scraping page 15\n",
            "Scraping page 16\n",
            "Scraping page 17\n",
            "Scraping page 18\n",
            "Scraping page 19\n",
            "Scraping page 20\n",
            "Scraping and exporting complete.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import time\n",
        "\n",
        "# Function to scrape product details from a listing page\n",
        "def scrape_listing_page(url):\n",
        "    headers = ({'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36', 'Accept-Language': 'en-US, en;q=0.5'})\n",
        "    response = requests.get(url, headers=headers)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    products = []\n",
        "    for product in soup.find_all('div', {'data-component-type': 's-search-result'}):\n",
        "        product_data = {}\n",
        "\n",
        "        # Scrape product details\n",
        "        product_name_element = product.find('span', {'class': 'a-text-normal'})\n",
        "        if product_name_element:\n",
        "            product_name = product_name_element.text.strip()\n",
        "            product_data['product_name'] = product_name\n",
        "\n",
        "        product_url_element = product.find('a', {'class': 'a-link-normal'})\n",
        "        if product_url_element:\n",
        "            product_url = 'https://www.amazon.in' + product_url_element['href']\n",
        "            product_data['product_url'] = product_url\n",
        "\n",
        "        product_price_element = product.find('span', {'class': 'a-price-whole'})\n",
        "        if product_price_element:\n",
        "            product_price = product_price_element.text.strip()\n",
        "            product_data['product_price'] = product_price\n",
        "\n",
        "        rating_element = product.find('span', {'class': 'a-icon-alt'})\n",
        "        if rating_element:\n",
        "            rating = rating_element.text.split()[0]\n",
        "            product_data['rating'] = rating\n",
        "\n",
        "        num_reviews_element = product.find('span', {'class': 'a-size-base'})\n",
        "        if num_reviews_element:\n",
        "            num_reviews = num_reviews_element.text.strip()\n",
        "            product_data['num_reviews'] = num_reviews\n",
        "\n",
        "        products.append(product_data)\n",
        "\n",
        "    return products\n",
        "\n",
        "# Function to scrape additional details from a product page\n",
        "def scrape_product_page(product_url):\n",
        "    headers = ({'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36', 'Accept-Language': 'en-US, en;q=0.5'})\n",
        "    response = requests.get(product_url, headers=headers)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    product_data = {}\n",
        "\n",
        "    description_element = soup.find('div', {'id': 'productDescription_feature_div'})\n",
        "    if description_element:\n",
        "        description = description_element.text.strip()\n",
        "        product_data['description'] = description\n",
        "\n",
        "    asin_element = soup.find('th', string='ASIN')\n",
        "    if asin_element:\n",
        "        asin = asin_element.find_next('td').text.strip()\n",
        "        product_data['asin'] = asin\n",
        "\n",
        "    manufacturer_element = soup.find('th', string='Manufacturer')\n",
        "    if manufacturer_element:\n",
        "        manufacturer = manufacturer_element.find_next('td').text.strip()\n",
        "        product_data['manufacturer'] = manufacturer\n",
        "\n",
        "    return product_data\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    base_url = \"https://www.amazon.in/s?k=bags&crid=2M096C61O4MLT&qid=1653308124&sprefix=ba%2Caps%2C283&ref=sr_pg_1\"\n",
        "    all_products = []\n",
        "\n",
        "    for page_num in range(1, 21):  # Scraping 20 pages\n",
        "        page_url = base_url + f'&page={page_num}'\n",
        "        print(f\"Scraping page {page_num}\")\n",
        "\n",
        "        products = scrape_listing_page(page_url)\n",
        "\n",
        "        for product in products:\n",
        "            if 'product_url' in product:\n",
        "                product_url = product['product_url']\n",
        "                product_details = scrape_product_page(product_url)\n",
        "                product.update(product_details)\n",
        "                all_products.append(product)\n",
        "\n",
        "        time.sleep(2)  # Add a delay to be respectful of the website's resources\n",
        "\n",
        "    # Export data to CSV\n",
        "    with open('amazon_products.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        fieldnames = ['product_name', 'product_url', 'product_price', 'rating', 'num_reviews', 'description', 'asin', 'manufacturer']\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "        writer.writeheader()\n",
        "        for product in all_products:\n",
        "            writer.writerow(product)\n",
        "\n",
        "    print(\"Scraping and exporting complete.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "As963u8jCEpd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}